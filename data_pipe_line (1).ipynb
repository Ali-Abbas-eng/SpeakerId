{"cells":[{"cell_type":"markdown","metadata":{},"source":["The dataset here is the features extracted from the Voxceleb 1 dataset using the DAE.\n","\n","This dataset was stored in numpy arrays so in npy files.\n","\n","For performance issues we switch to a better method using the tfrecords.\n","\n","So in this notebook we will copy the dataset to tfrecords files."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6735,"status":"ok","timestamp":1634975120067,"user":{"displayName":"Kais Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQmD3R6mxgiMbBghfUfTeCwvIEA4WsjO8q_2JQuQ=s64","userId":"11045491961428007438"},"user_tz":-180},"id":"aWr9-Zvpt87d"},"outputs":[],"source":["import numpy as np\n","import gc\n","import tensorflow as tf\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80406,"status":"ok","timestamp":1634969981855,"user":{"displayName":"Kais Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQmD3R6mxgiMbBghfUfTeCwvIEA4WsjO8q_2JQuQ=s64","userId":"11045491961428007438"},"user_tz":-180},"id":"TT5mWVx9uFpS","outputId":"a8460c50-88c3-4d7f-cddd-5761f8885156"},"outputs":[],"source":["# Mount google drive to colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1634966812017,"user":{"displayName":"Kais Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQmD3R6mxgiMbBghfUfTeCwvIEA4WsjO8q_2JQuQ=s64","userId":"11045491961428007438"},"user_tz":-180},"id":"tGAKoRZOuF16","outputId":"5316a628-d174-46bc-f591-7180f5a9ce2f"},"outputs":[],"source":["# Get the paths of the files that contains the featuers.\n","dataset_files = []\n","for i in range(7):\n","  file = f'/content/drive/MyDrive/30K_vox{i+1}.npy'\n","  dataset_files.append(file)\n","print(dataset_files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write function\n","def record_write(path, x, y):\n","  with tf.io.TFRecordWriter(path, 'GZIP') as file_writer:\n","    for i in range(x.shape[0]):\n","      record_bytes = tf.train.Example(features=tf.train.Features(feature={\n","          \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=x[i].flatten())),\n","          \"y\": tf.train.Feature(float_list=tf.train.FloatList(value=y[i])),\n","      })).SerializeToString()\n","      file_writer.write(record_bytes)\n","\n","def record_read(record_bytes):\n","  parsed_features =  tf.io.parse_single_example(\n","      # Data\n","      record_bytes,\n","\n","      # Schema\n","      {\"x\": tf.io.FixedLenFeature([58*32*16], dtype=tf.float32),\n","       \"y\": tf.io.FixedLenFeature([1251], dtype=tf.float32)}\n","  )\n","  x = parsed_features['x']\n","  x = tf.reshape(x, (58, 32, 16))\n","  y = parsed_features['y']\n","  return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2344701,"status":"ok","timestamp":1634969272188,"user":{"displayName":"Kais Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQmD3R6mxgiMbBghfUfTeCwvIEA4WsjO8q_2JQuQ=s64","userId":"11045491961428007438"},"user_tz":-180},"id":"XQ7pGIzYuGGu","outputId":"e2744f64-82b2-474c-a7c9-ed5926e1f196"},"outputs":[],"source":["# Loop through the files.\n","for f in range(len(dataset_files)):\n","  ############### Data Pipeline ##################################\n","  gc.collect()\n","  print(f'start loadng part: {f+1}, which is: {dataset_files[f]}')\n","\n","  # Load the numpy array\n","  data = np.load(dataset_files[f], allow_pickle=True)\n","\n","  # Each npy file contains a numpy array of objects\n","  # There are two columns, the first one contains the examples, and the second contains the labels\n","  x_train_copy =  np.array(data[:, 0].copy())\n","  #print(x_train_copy.shape)\n","  y_train_copy = np.array(data[:, 1].copy())\n","  #print(y_train_copy.shape)\n","  del data\n","  gc.collect()\n","\n","  # Reshape the arrays to the original shape and specify the type (since we have object type until now)\n","  x_train = np.zeros(\n","      (x_train_copy.shape[0], 58, 32, 16),\n","      dtype=np.float32\n","  )\n","  for i in range(0, x_train.shape[0]):\n","      x_train[i] = np.array(\n","          x_train_copy[i],\n","          dtype=np.float32\n","      ).reshape(58, 32, 16)\n","  # x_train = x_train[..., np.newaxis]\n","  print(x_train.shape)\n","\n","  y_train = np.zeros(\n","      (y_train_copy.shape[0], y_train_copy[0].shape[0]),\n","      dtype=np.float32\n","  )\n","\n","  for i in range(0, y_train.shape[0]):\n","      y_train[i] = np.array(y_train_copy[i], dtype=np.float32)\n","  print(y_train.shape)\n","\n","\n","  del x_train_copy\n","  del y_train_copy\n","  gc.collect()\n","  ############### Data Pipeline ##################################\n","  ############### Data Storing ###################################\n","  \n","  # It is recommended to have 100MB file size.\n","  # So for our case this happens with about 1024 file.\n","  # Powers of 2 for better performance.\n","  FILE_SIZE = (1<<10)\n","  for i in tqdm(range(1, 19 + 1)):\n","    st = (i-1)*FILE_SIZE\n","    en = i*FILE_SIZE\n","    file_path = f'/content/drive/MyDrive/dataset/vox_part{f}.{i}.tfrecords'\n","    # create the file\n","    open(file_path, 'a').close()\n","    # write to the file\n","    record_write(file_path, x_train[st:en], y_train[st:en])\n","\n","  ############### Data Storing ###################################\n","  del x_train\n","  del y_train\n","  gc.collect()\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNJzc3oJhtTH4QRnGE7FLcx","collapsed_sections":[],"name":"data_pipe_line.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
